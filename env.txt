# ===============================
# Environment Configuration (env.txt)
# ===============================
# Qdrant Server
QDRANT_URL=http://192.168.0.224:6333
# (optional) Qdrant API key
QDRANT_KEY=

# Embedding Model (SentenceTransformers)
EMBED_MODEL=bge-m3:latest

# Ollama Inference (optional,優先使用 Ollama)
OLLAMA_HOST=http://192.168.0.224:11434
OLLAMA_MODEL=deepseek-coder:6.7b

# OpenAI-compatible API (若不使用 Ollama，則同時設定以下三項)
LLM_KEY=
LLM_BASE=
LLM_MODEL=

# Performance and retrieval parameters
# texts per embed call (gpu batch size)
EMBED_BATCH=192
# points per Qdrant upsert chunk
QDRANT_UPSERT_CHUNK=1024
# 1=resume from previous index state
RESUME=1

# AST/function chunk settings during build
# max lines per function chunk in /build_rag
MAX_FUNC_LINES_BUILD=80
# skip trivial (<3 lines) functions in build
SKIP_TRIVIAL_FUNCS=1
MAX_BYTES_PER_SNIPPET=4096
MAX_POINTS_PER_FILE=800

# Rerank & packing settings
# cross-encoder model for reranking
RERANK_MODEL=bge-reranker-base
# rerank top K candidates
RERANK_TOP_K=36
# token limit when packing context
PACK_TOKEN_LIMIT=1400

# On-demand deepening settings
# 1=enable AST-aware deepening in /ask_rag
DEEPEN_ENABLE=1
# number of files to deepen per query
DEEPEN_TOP_FILES=3
# max lines per function for on-demand deepening
MAX_FUNC_LINES=500
# target lines per function chunk
FUNC_CHUNK_TARGET=220
# overlap lines when chunking functions
FUNC_CHUNK_OVERLAP=12

# State directory for indexes and caches
INDEX_STATE_DIR=.rag_state
